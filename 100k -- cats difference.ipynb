{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c7b5bdeb-e811-41d5-9a12-06dfb502909e",
   "metadata": {},
   "outputs": [],
   "source": [
    "%reload_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "187e1431-06c4-49ee-b879-00fefd16143b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "import h5py\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn import preprocessing\n",
    "import os\n",
    "import time\n",
    "from pathlib import Path\n",
    "from urllib.request import urlretrieve\n",
    "import logging\n",
    "from li.Baseline import Baseline\n",
    "from li.LearnedIndex import LearnedIndex\n",
    "from li.utils import save_as_pickle\n",
    "from li.model import data_X_to_torch\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "22456ea8-b20d-4e62-8543-25fb7b3e7b19",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(2023)\n",
    "\n",
    "logging.basicConfig(\n",
    "    level=logging.INFO,\n",
    "    format='[%(asctime)s][%(levelname)-5.5s][%(name)-.20s] %(message)s'\n",
    ")\n",
    "LOG = logging.getLogger(__name__)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a9290134-9fa9-472a-8a5e-519633d52797",
   "metadata": {},
   "outputs": [],
   "source": [
    "kind='pca32v2'\n",
    "size='100K'\n",
    "key='pca32'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b72713b8-329f-4484-9b2e-49a3819dee7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = np.array(h5py.File(os.path.join(\"data\", kind, size, \"dataset.h5\"), \"r\")[key])\n",
    "queries = np.array(h5py.File(os.path.join(\"data\", kind, size, \"query.h5\"), \"r\")[key])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3d0d2ba0-8af2-41d3-ab17-1d9bb23c835c",
   "metadata": {},
   "outputs": [],
   "source": [
    "kind_search = 'clip768v2'\n",
    "key_search = 'emb'\n",
    "data_search = np.array(\n",
    "    h5py.File(os.path.join(\"data\", kind_search, size, \"dataset.h5\"), \"r\")[key_search]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "96bb39a7-7ca1-4b29-92d6-a32bf4cad2bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "queries_search = np.array(\n",
    "    h5py.File(os.path.join(\"data\", kind_search, size, \"query.h5\"), \"r\")[key_search]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "01d78667-13c9-4564-a73a-b27e4dca7dd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "import torch.nn.functional as nnf\n",
    "import numpy as np\n",
    "from li.Logger import Logger\n",
    "from typing import Tuple\n",
    "import torch.utils.data\n",
    "\n",
    "torch.manual_seed(2023)\n",
    "np.random.seed(2023)\n",
    "\n",
    "class Model(nn.Module):\n",
    "    def __init__(self, input_dim=768, output_dim=1000, model_type=None):\n",
    "        super().__init__()\n",
    "        if model_type == 'MLP':\n",
    "            self.layers = torch.nn.Sequential(\n",
    "                torch.nn.Linear(input_dim, 128),\n",
    "                torch.nn.ReLU(),\n",
    "                torch.nn.Linear(128, output_dim)\n",
    "            )\n",
    "        if model_type == 'MLP-2':\n",
    "            self.layers = torch.nn.Sequential(\n",
    "                torch.nn.Linear(input_dim, 64),\n",
    "                torch.nn.ReLU(),\n",
    "                torch.nn.Linear(64, output_dim)\n",
    "            )\n",
    "        if model_type == 'MLP-3':\n",
    "            self.layers = torch.nn.Sequential(\n",
    "                torch.nn.Linear(input_dim, 256),\n",
    "                torch.nn.ReLU(),\n",
    "                torch.nn.Linear(256, output_dim)\n",
    "            )\n",
    "        if model_type == 'MLP-4':\n",
    "            self.layers = torch.nn.Sequential(\n",
    "                torch.nn.Linear(input_dim, 512),\n",
    "                torch.nn.ReLU(),\n",
    "                torch.nn.Linear(512, output_dim)\n",
    "            )\n",
    "        if model_type == 'MLP-5':\n",
    "            self.layers = torch.nn.Sequential(\n",
    "                torch.nn.Linear(input_dim, 256),\n",
    "                torch.nn.ReLU(),\n",
    "                torch.nn.Linear(256, 128),\n",
    "                torch.nn.ReLU(),\n",
    "                torch.nn.Linear(128, output_dim)\n",
    "            )\n",
    "        if model_type == 'MLP-6':\n",
    "            self.layers = torch.nn.Sequential(\n",
    "                torch.nn.Linear(input_dim, 32),\n",
    "                torch.nn.ReLU(),\n",
    "                torch.nn.Linear(32, output_dim)\n",
    "            )\n",
    "        if model_type == 'MLP-7':\n",
    "            self.layers = torch.nn.Sequential(\n",
    "                torch.nn.Linear(input_dim, 16),\n",
    "                torch.nn.ReLU(),\n",
    "                torch.nn.Linear(16, output_dim)\n",
    "            )\n",
    "        if model_type == 'MLP-8':\n",
    "            self.layers = torch.nn.Sequential(\n",
    "                torch.nn.Linear(input_dim, 8),\n",
    "                torch.nn.ReLU(),\n",
    "                torch.nn.Linear(8, output_dim)\n",
    "            )\n",
    "        if model_type == 'MLP-9':\n",
    "            self.layers = torch.nn.Sequential(\n",
    "                torch.nn.Linear(input_dim, 8),\n",
    "                torch.nn.ReLU(),\n",
    "                torch.nn.Linear(input_dim, 16),\n",
    "                torch.nn.ReLU(),\n",
    "                torch.nn.Linear(16, output_dim)\n",
    "            )\n",
    "        self.n_output_neurons = output_dim\n",
    "\n",
    "    def forward(self, x: torch.FloatTensor) -> torch.FloatTensor:\n",
    "        outputs = self.layers(x)\n",
    "        return outputs\n",
    "\n",
    "\n",
    "def data_X_to_torch(data) -> torch.FloatTensor:\n",
    "    \"\"\" Creates torch training data.\"\"\"\n",
    "    data_X = torch.from_numpy(np.array(data).astype(np.float32))\n",
    "    return data_X\n",
    "\n",
    "\n",
    "def data_to_torch(data, labels) -> Tuple[torch.FloatTensor, torch.LongTensor]:\n",
    "    \"\"\" Creates torch training data and labels.\"\"\"\n",
    "    data_X = data_X_to_torch(data)\n",
    "    data_y = torch.as_tensor(torch.from_numpy(labels), dtype=torch.long)\n",
    "    return data_X, data_y\n",
    "\n",
    "\n",
    "def get_device() -> torch.device:\n",
    "    \"\"\" Gets the `device` to be used by torch.\n",
    "    This arugment is needed to operate with the PyTorch model instance.\n",
    "\n",
    "    Returns\n",
    "    ------\n",
    "    torch.device\n",
    "        Device\n",
    "    \"\"\"\n",
    "    use_cuda = torch.cuda.is_available()\n",
    "    device = torch.device('cuda:0' if use_cuda else 'cpu')\n",
    "    torch.backends.cudnn.benchmark = True\n",
    "    return device\n",
    "\n",
    "\n",
    "class NeuralNetwork(Logger):\n",
    "    \"\"\" The neural network class corresponding to every inner node.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    input_dim : int\n",
    "        The input dimension.\n",
    "    output_dim : int\n",
    "        The output dimension.\n",
    "    loss : torch.nn, optional\n",
    "        The loss function, the default is torch.nn.CrossEntropyLoss.\n",
    "    lr : float, optional\n",
    "        The learning rate, the default is 0.001.\n",
    "    model_type : str, optional\n",
    "        The model type, the default is 'MLP'.\n",
    "    class_weight : torch.FloatTensor, optional\n",
    "        The class weights, the default is None.\n",
    "    \"\"\"\n",
    "    def __init__(\n",
    "        self,\n",
    "        input_dim,\n",
    "        output_dim,\n",
    "        loss=torch.nn.CrossEntropyLoss,\n",
    "        lr=0.1,\n",
    "        model_type='MLP',\n",
    "        class_weight=None\n",
    "    ):\n",
    "        self.device = get_device()\n",
    "        self.model = Model(input_dim, output_dim, model_type=model_type).to(self.device)\n",
    "        if not isinstance(class_weight, type(None)):\n",
    "            self.loss = loss(weight=class_weight.to(self.device))\n",
    "        else:\n",
    "            self.loss = loss()\n",
    "        self.optimizer = torch.optim.Adam(self.model.parameters(), lr=lr)\n",
    "\n",
    "    def train(\n",
    "        self,\n",
    "        data_X: torch.FloatTensor,\n",
    "        data_y: torch.LongTensor,\n",
    "        epochs=500,\n",
    "        logger=None\n",
    "    ):\n",
    "        step = epochs // 10\n",
    "        losses = []\n",
    "        if logger:\n",
    "            logger.info(f'Epochs: {epochs}, step: {step}')\n",
    "        for ep in range(epochs):\n",
    "            pred_y = self.model(data_X.to(self.device))\n",
    "            curr_loss = self.loss(pred_y, data_y.to(self.device))\n",
    "            if ep % step == 0 and ep != 0:\n",
    "                if logger:\n",
    "                    logger.info(f'Epoch {ep} | Loss {curr_loss.item()}')\n",
    "            losses.append(curr_loss.item())\n",
    "\n",
    "            self.model.zero_grad()\n",
    "            curr_loss.backward()\n",
    "\n",
    "            self.optimizer.step()\n",
    "        return losses\n",
    "\n",
    "    def train_batch(\n",
    "        self,\n",
    "        dataset,\n",
    "        epochs=5,\n",
    "        logger=None\n",
    "    ):\n",
    "        step = epochs // 10\n",
    "        step = step if step > 0 else 1\n",
    "        losses = []\n",
    "        if logger:\n",
    "            logger.info(f'Epochs: {epochs}, step: {step}')\n",
    "        for ep in range(epochs):\n",
    "            for data_X, data_y in iter(dataset):\n",
    "                pred_y = self.model(data_X.to(self.device))\n",
    "                curr_loss = self.loss(pred_y, data_y.to(self.device))\n",
    "\n",
    "            if ep % step == 0 and ep != 0:\n",
    "                if logger:\n",
    "                    logger.info(f'Epoch {ep} | Loss {curr_loss.item():.5f}')\n",
    "            losses.append(curr_loss.item())\n",
    "\n",
    "            self.model.zero_grad()\n",
    "            curr_loss.backward()\n",
    "\n",
    "            self.optimizer.step()\n",
    "        return losses\n",
    "\n",
    "    def predict(self, data_X: torch.FloatTensor):\n",
    "        \"\"\" Collects predictions for multiple data points (used in structure building).\"\"\"\n",
    "        self.model = self.model.to(self.device)\n",
    "        self.model.eval()\n",
    "\n",
    "        all_outputs = torch.tensor([], device=self.device)\n",
    "        with torch.no_grad():\n",
    "            outputs = self.model(data_X.to(self.device))\n",
    "            all_outputs = torch.cat((all_outputs, outputs), 0)\n",
    "\n",
    "        _, y_pred = torch.max(all_outputs, 1)\n",
    "        return y_pred.cpu().numpy()\n",
    "\n",
    "    def predict_proba(self, data_X: torch.FloatTensor):\n",
    "        \"\"\" Collects predictions for a single data point (used in query predictions).\"\"\"\n",
    "        self.model = self.model.to(self.device)\n",
    "        self.model.eval()\n",
    "\n",
    "        with torch.no_grad():\n",
    "            outputs = self.model(data_X.to(self.device))\n",
    "\n",
    "        if outputs.dim() == 1:\n",
    "            dim = 0\n",
    "        else:\n",
    "            dim = 1\n",
    "        prob = nnf.softmax(outputs, dim=dim)\n",
    "        probs, classes = prob.topk(prob.shape[1])\n",
    "\n",
    "        return probs.cpu().numpy(), classes.cpu().numpy()\n",
    "\n",
    "\n",
    "class LIDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, dataset_x, dataset_y):\n",
    "        self.dataset_x, self.dataset_y = data_to_torch(dataset_x, dataset_y)\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.dataset_x.shape[0]\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.dataset_x[idx-1], self.dataset_y[idx-1]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b60be92e-0fdb-4b37-bec8-168843a043a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import numpy as np\n",
    "import time\n",
    "\n",
    "def pairwise_cosine_threshold(x, y, threshold, cat_idxs, k=10):\n",
    "    s = time.time()\n",
    "    result = 1-cosine_similarity(x, y)\n",
    "    t_pure_seq_search = time.time() - s\n",
    "    # create an array of consisten shapes\n",
    "    #print(result.shape)\n",
    "    #print(threshold.shape, threshold[cat_idxs].shape)\n",
    "    thresh_consistent = np.repeat(threshold[cat_idxs, np.newaxis], result.shape[1], 1)\n",
    "    relevant_dists = np.where(result < thresh_consistent)\n",
    "    # filter the relevant object ids\n",
    "    try:\n",
    "        relevant_object_ids = np.unique(relevant_dists[1])\n",
    "        max_idx = relevant_object_ids.shape[0]\n",
    "        if max_idx == 0:\n",
    "            return None, t_pure_seq_search\n",
    "    except ValueError:\n",
    "        # There is no distance below the threshold, we can return\n",
    "        return None, t_pure_seq_search\n",
    "    max_idx = max_idx if max_idx > k else k\n",
    "    # output array filled with some large value\n",
    "    output_arr = np.full(shape=(result.shape[0], max_idx), fill_value=10_000, dtype=np.float)\n",
    "    #index_df = pd.DataFrame(relevant_dists[0])\n",
    "    #index_df = pd.DataFrame(relevant_dists[0], relevant_dists[1]).reset_index()\n",
    "    # create indexes to store the relevant distances\n",
    "    #index_df['mapping'] = index_df.groupby('index').ngroup()\n",
    "    mapping = dict(zip(relevant_object_ids, np.arange(relevant_object_ids.shape[0])))\n",
    "    # tried also with np.vectorize, wasn't faster\n",
    "    output_arr_2nd_dim = np.array([mapping[x] for x in relevant_dists[1]])\n",
    "    to_be_added = result[relevant_dists[0], relevant_dists[1]]\n",
    "    # populate the output array\n",
    "    output_arr[relevant_dists[0], output_arr_2nd_dim] = to_be_added\n",
    "    return output_arr, relevant_object_ids, t_pure_seq_search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c9ea228f-0c7b-480f-be0a-ccef4a35ef30",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2023-07-14 15:11:32,229][INFO ][faiss.loader] Loading faiss with AVX2 support.\n",
      "[2023-07-14 15:11:32,232][INFO ][faiss.loader] Could not load library with AVX2 support due to:\n",
      "ModuleNotFoundError(\"No module named 'faiss.swigfaiss_avx2'\",)\n",
      "[2023-07-14 15:11:32,234][INFO ][faiss.loader] Loading faiss.\n",
      "[2023-07-14 15:11:32,265][INFO ][faiss.loader] Successfully loaded faiss.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from li.Logger import Logger\n",
    "from li.utils import pairwise_cosine\n",
    "import time\n",
    "import torch\n",
    "import torch.utils.data\n",
    "import faiss\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "torch.manual_seed(2023)\n",
    "np.random.seed(2023)\n",
    "\n",
    "class LearnedIndex(Logger):\n",
    "\n",
    "    def __init__(self):\n",
    "        self.pq = []\n",
    "        self.model = None\n",
    "\n",
    "    def search(\n",
    "        self,\n",
    "        data_navigation,\n",
    "        queries_navigation,\n",
    "        data_search,\n",
    "        queries_search,\n",
    "        pred_categories,\n",
    "        n_buckets=1,\n",
    "        k=10,\n",
    "        use_threshold=False\n",
    "    ):\n",
    "        \"\"\" Search for k nearest neighbors for each query in queries.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        queries : np.array\n",
    "            Queries to search for.\n",
    "        data : np.array\n",
    "            Data to search in.\n",
    "        n_buckets : int\n",
    "            Number of most similar buckets to search in.\n",
    "        k : int\n",
    "            Number of nearest neighbors to search for.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        dists : np.array\n",
    "            Array of shape (queries.shape[0], k) with distances to nearest neighbors for each query.\n",
    "        anns : np.array\n",
    "            Array of shape (queries.shape[0], k) with nearest neighbors for each query.\n",
    "        time : float\n",
    "            Time it took to search.\n",
    "        \"\"\"\n",
    "        assert self.model is not None, 'Model is not trained, call `build` first.'\n",
    "        s = time.time()\n",
    "        _, pred_proba_categories = self.model.predict_proba(\n",
    "            data_X_to_torch(queries_navigation)\n",
    "        )\n",
    "        t_inference = time.time() - s\n",
    "        anns_final = None\n",
    "        dists_final = None\n",
    "        # sorts the predictions of a bucket for each query, ordered by lowest probability\n",
    "        data_navigation['category'] = pred_categories\n",
    "\n",
    "        # iterates over the predicted buckets starting from the most similar (index -1)\n",
    "        t_all_buckets = 0\n",
    "        t_all_pairwise = 0\n",
    "        t_all_sort = 0\n",
    "        t_all_pure_pairwise = 0\n",
    "        t_comp_threshold = 0\n",
    "        for bucket in range(n_buckets):\n",
    "            if bucket != 0 and use_threshold:\n",
    "                s_ = time.time()\n",
    "                threshold_dist = dists_final.max(axis=1)\n",
    "                t_comp_threshold += time.time() - s_\n",
    "            else:\n",
    "                threshold_dist = None\n",
    "            dists, anns, t_all, t_pairwise, t_pure_pairwise, t_sort = self.search_single(\n",
    "                data_navigation,\n",
    "                data_search,\n",
    "                queries_search,\n",
    "                pred_proba_categories[:, bucket],\n",
    "                threshold_dist=threshold_dist\n",
    "            )\n",
    "            t_all_buckets += t_all\n",
    "            t_all_pairwise += t_pairwise\n",
    "            t_all_pure_pairwise += t_pure_pairwise\n",
    "            t_all_sort += t_sort\n",
    "            if anns_final is None:\n",
    "                anns_final = anns\n",
    "                dists_final = dists\n",
    "            else:\n",
    "                # stacks the results from the previous sorted anns and dists\n",
    "                # *_final arrays now have shape (queries.shape[0], k*2)\n",
    "                anns_final = np.hstack((anns_final, anns))\n",
    "                dists_final = np.hstack((dists_final, dists))\n",
    "                # gets the sorted indices of the stacked dists\n",
    "                idx_sorted = dists_final.argsort(kind='stable', axis=1)[:, :k]\n",
    "                # indexes the final arrays with the sorted indices\n",
    "                # *_final arrays now have shape (queries.shape[0], k)\n",
    "                idx = np.ogrid[tuple(map(slice, dists_final.shape))]\n",
    "                idx[1] = idx_sorted\n",
    "                dists_final = dists_final[tuple(idx)]\n",
    "                anns_final = anns_final[tuple(idx)]\n",
    "\n",
    "                assert anns_final.shape == dists_final.shape == (queries_search.shape[0], k)\n",
    "\n",
    "        self.logger.info(f't_comp_threshold: {t_comp_threshold}')\n",
    "        return dists_final, anns_final, time.time() - s, t_inference, t_all_buckets, t_all_pairwise, t_all_pure_pairwise, t_all_sort\n",
    "\n",
    "    def search_single(\n",
    "        self,\n",
    "        data_navigation,\n",
    "        data_search,\n",
    "        queries_search,\n",
    "        pred_categories,\n",
    "        k=10,\n",
    "        threshold_dist=None\n",
    "    ):\n",
    "        \"\"\" Search for k nearest neighbors for each query in queries.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        queries : np.array\n",
    "            Queries to search for.\n",
    "        data : np.array\n",
    "            Data to search in.\n",
    "        k : int\n",
    "            Number of nearest neighbors to search for.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        anns : np.array\n",
    "            Array of shape (queries.shape[0], k) with nearest neighbors for each query.\n",
    "        final_dists_k : np.array\n",
    "            Array of shape (queries.shape[0], k) with distances to nearest neighbors for each query.\n",
    "        time : float\n",
    "            Time it took to search.\n",
    "        \"\"\"\n",
    "        s_all = time.time()\n",
    "        nns = np.zeros((queries_search.shape[0], k), dtype=np.uint32)\n",
    "        dists = np.zeros((queries_search.shape[0], k), dtype=np.float32)\n",
    "\n",
    "        if 'category' in data_search.columns:\n",
    "            data_search = data_search.drop('category', axis=1, errors='ignore')\n",
    "\n",
    "        t_pairwise = 0\n",
    "        t_pure_pairwise = 0\n",
    "        t_sort = 0\n",
    "        for cat, g in tqdm(data_navigation.groupby('category')):\n",
    "            cat_idxs = np.where(pred_categories == cat)[0]\n",
    "            bucket_obj_indexes = g.index\n",
    "            if bucket_obj_indexes.shape[0] != 0 and cat_idxs.shape[0] != 0:\n",
    "                s = time.time()\n",
    "                # TODO: Add filter, filter will be different for every query\n",
    "                # OR pass nns, dists from previous buckets\n",
    "                if threshold_dist is not None:\n",
    "                    seq_search_dists = pairwise_cosine_threshold(\n",
    "                        queries_search[cat_idxs],\n",
    "                        data_search.loc[bucket_obj_indexes],\n",
    "                        threshold_dist,\n",
    "                        cat_idxs,\n",
    "                        k\n",
    "                    )\n",
    "                    if seq_search_dists[0] is None:\n",
    "                        t_pure_pairwise += seq_search_dists[1]\n",
    "                        # There is no distance below the threshold, we can continue\n",
    "                        continue\n",
    "                    else:\n",
    "                        # seq_search_dists[1] contains the indexes of the relevant objects\n",
    "                        bucket_obj_indexes = bucket_obj_indexes[seq_search_dists[1]]\n",
    "                        t_pure_pairwise += seq_search_dists[2]\n",
    "                        seq_search_dists = seq_search_dists[0]\n",
    "                else:\n",
    "                    s_ = time.time()\n",
    "                    seq_search_dists = pairwise_cosine(\n",
    "                        queries_search[cat_idxs],\n",
    "                        data_search.loc[bucket_obj_indexes]\n",
    "                    )\n",
    "                    t_pure_pairwise += time.time() - s_\n",
    "                t_pairwise += time.time() - s\n",
    "                s = time.time()\n",
    "                ann_relative = seq_search_dists.argsort(kind='quicksort')[\n",
    "                    :, :k if k < seq_search_dists.shape[1] else seq_search_dists.shape[1]\n",
    "                ]\n",
    "                t_sort += time.time() - s\n",
    "                if bucket_obj_indexes.shape[0] < k:\n",
    "                    # pad to `k` if needed\n",
    "                    pad_needed = (k - bucket_obj_indexes.shape[0]) // 2 + 1\n",
    "                    bucket_obj_indexes = np.pad(np.array(bucket_obj_indexes), pad_needed, 'edge')[:k]\n",
    "                    ann_relative = np.pad(ann_relative[0], pad_needed, 'edge')[:k].reshape(1, -1)\n",
    "                    seq_search_dists = np.pad(seq_search_dists[0], pad_needed, 'edge')[:k].reshape(1, -1)\n",
    "                    _, i = np.unique(seq_search_dists, return_index=True)\n",
    "                    duplicates_i = np.setdiff1d(np.arange(k), i)\n",
    "                    # assign a large number such that the duplicated value gets replaced\n",
    "                    seq_search_dists[0][duplicates_i] = 10_000\n",
    "\n",
    "                nns[cat_idxs] = np.array(bucket_obj_indexes)[ann_relative]\n",
    "                dists[cat_idxs] = np.take_along_axis(seq_search_dists, ann_relative, axis=1)\n",
    "        t_all = time.time() - s_all\n",
    "        return dists, nns, t_all, t_pairwise, t_pure_pairwise, t_sort\n",
    "\n",
    "\n",
    "    def build(self, data, n_categories=100, epochs=100, lr=0.1, model_type='MLP'):\n",
    "        \"\"\" Build the index.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        data : np.array\n",
    "            Data to build the index on.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        time : float\n",
    "            Time it took to build the index.\n",
    "        \"\"\"\n",
    "        s = time.time()\n",
    "        # ---- cluster the data into categories ---- #\n",
    "        _, labels = self.cluster(data, n_categories)\n",
    "\n",
    "        # ---- train a neural network ---- #\n",
    "        dataset = LIDataset(data, labels)\n",
    "        train_loader = torch.utils.data.DataLoader(\n",
    "            dataset,\n",
    "            batch_size=256,\n",
    "            sampler=torch.utils.data.SubsetRandomSampler(\n",
    "                data.index.values.tolist()\n",
    "            )\n",
    "        )\n",
    "        nn = NeuralNetwork(\n",
    "            input_dim=data.shape[1],\n",
    "            output_dim=n_categories,\n",
    "            lr=lr,\n",
    "            model_type=model_type\n",
    "        )\n",
    "        nn.train_batch(train_loader, epochs=epochs, logger=self.logger)\n",
    "        # ---- collect predictions ---- #\n",
    "        self.model = nn\n",
    "        return nn.predict(data_X_to_torch(data)), time.time() - s\n",
    "\n",
    "    def cluster(\n",
    "        self,\n",
    "        data,\n",
    "        n_clusters,\n",
    "        n_redo=10,\n",
    "        spherical=True,\n",
    "        int_centroids=True,\n",
    "\n",
    "    ):\n",
    "        if data.shape[0] < 2:\n",
    "            return None, np.zeros_like(data.shape[0])\n",
    "\n",
    "        if data.shape[0] < n_clusters:\n",
    "            n_clusters = data.shape[0] // 5\n",
    "            if n_clusters < 2:\n",
    "                n_clusters = 2\n",
    "\n",
    "        kmeans = faiss.Kmeans(\n",
    "            d=np.array(data).shape[1],\n",
    "            k=n_clusters,\n",
    "            verbose=True,\n",
    "            #nredo=n_redo,\n",
    "            #spherical=spherical,\n",
    "            #int_centroids=int_centroids,\n",
    "            #update_index=False,\n",
    "            seed=2023\n",
    "        )\n",
    "        X = np.array(data).astype(np.float32)\n",
    "        kmeans.train(X)\n",
    "\n",
    "        return kmeans, kmeans.index.search(X, 1)[1].T[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c7fdd91b-2b2b-4e5a-8236-fe4d04bce831",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.DataFrame(data)\n",
    "data.index += 1\n",
    "\n",
    "data_search = pd.DataFrame(data_search)\n",
    "data_search.index += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "8a4f67fd-3a56-4e97-a9b6-9de9ce1cdd85",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2023-07-14 15:06:06,034][INFO ][__main__.LearnedInde] Epochs: 200, step: 20\n",
      "[2023-07-14 15:06:29,323][INFO ][__main__.LearnedInde] Epoch 20 | Loss 1.98601\n",
      "[2023-07-14 15:06:51,398][INFO ][__main__.LearnedInde] Epoch 40 | Loss 1.18380\n",
      "[2023-07-14 15:07:14,368][INFO ][__main__.LearnedInde] Epoch 60 | Loss 1.05507\n",
      "[2023-07-14 15:07:36,406][INFO ][__main__.LearnedInde] Epoch 80 | Loss 1.10324\n",
      "[2023-07-14 15:07:58,435][INFO ][__main__.LearnedInde] Epoch 100 | Loss 1.13756\n",
      "[2023-07-14 15:08:21,092][INFO ][__main__.LearnedInde] Epoch 120 | Loss 0.89168\n",
      "[2023-07-14 15:08:43,176][INFO ][__main__.LearnedInde] Epoch 140 | Loss 1.07258\n",
      "[2023-07-14 15:09:05,576][INFO ][__main__.LearnedInde] Epoch 160 | Loss 0.81745\n",
      "[2023-07-14 15:09:27,546][INFO ][__main__.LearnedInde] Epoch 180 | Loss 0.70368\n"
     ]
    }
   ],
   "source": [
    "li = LearnedIndex()\n",
    "pred_categories, build_t = li.build(\n",
    "    data,\n",
    "    n_categories=200,\n",
    "    epochs=200,\n",
    "    lr=0.1\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "2f4bfd54-5391-412f-9ffe-6de9c65bd5de",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2023-07-14 15:11:36,772][INFO ][__main__] Loading GT\n"
     ]
    }
   ],
   "source": [
    "LOG.info(f'Loading GT')\n",
    "gt_path = f'data/groundtruth-{size}.h5'\n",
    "f3 = h5py.File(gt_path, 'r')\n",
    "loaded_gt = f3['knns'][:, :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "a9347709-15d0-4988-8695-a80e675868d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "k=10\n",
    "bucket=4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "e7275683-e332-47f2-911d-9500cfd5a122",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 200/200 [00:01<00:00, 135.27it/s]\n",
      "100%|██████████| 200/200 [00:01<00:00, 151.76it/s]\n",
      "100%|██████████| 200/200 [00:01<00:00, 166.24it/s]\n",
      "100%|██████████| 200/200 [00:01<00:00, 166.79it/s]\n",
      "[2023-07-14 15:10:20,767][INFO ][__main__.LearnedInde] t_comp_threshold: 0.0014171600341796875\n",
      "[2023-07-14 15:10:20,770][INFO ][__main__] Inference time: 0.16141343116760254\n",
      "[2023-07-14 15:10:20,773][INFO ][__main__] Search time: 5.416271448135376\n",
      "[2023-07-14 15:10:20,776][INFO ][__main__] Search single time: 5.240442276000977\n",
      "[2023-07-14 15:10:20,778][INFO ][__main__] Sequential search time: 4.479606866836548\n",
      "[2023-07-14 15:10:20,781][INFO ][__main__] Pure sequential search time: 3.7803361415863037\n",
      "[2023-07-14 15:10:20,783][INFO ][__main__] Sort time: 0.373150110244751\n"
     ]
    }
   ],
   "source": [
    "dists, nns, search_t, inference_t, search_single_t, seq_search_t, pure_seq_search_t, sort_t = li.search(\n",
    "    data_navigation=data,\n",
    "    queries_navigation=queries,\n",
    "    data_search=data_search,\n",
    "    queries_search=queries_search,\n",
    "    pred_categories=pred_categories,\n",
    "    n_buckets=bucket,\n",
    "    k=k,\n",
    "    use_threshold=True\n",
    ")\n",
    "LOG.info('Inference time: %s', inference_t)\n",
    "LOG.info('Search time: %s', search_t)\n",
    "LOG.info('Search single time: %s', search_single_t)\n",
    "LOG.info('Sequential search time: %s', seq_search_t)\n",
    "LOG.info('Pure sequential search time: %s', pure_seq_search_t)\n",
    "LOG.info('Sort time: %s', sort_t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "a7146c3b-62ea-443d-aa72-781a246b164e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_recall(I, gt, k):\n",
    "    assert k <= I.shape[1]\n",
    "    assert len(I) == len(gt)\n",
    "\n",
    "    n = len(I)\n",
    "    recall = 0\n",
    "    for i in range(n):\n",
    "        recall += len(set(I[i, :k]) & set(gt[i, :k]))\n",
    "    return recall / (n * k)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "13d1de50-4b74-4d45-8d93-eb6c075d842a",
   "metadata": {},
   "outputs": [],
   "source": [
    "recall = get_recall(nns, loaded_gt, 10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "c73ba8ce-1428-4f51-bf49-4a7aa90d37b7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.70667"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "recall"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dccedc46-2d8e-44a4-bb82-a31c4c036b5b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2023-07-14 15:11:44,421][INFO ][__main__.LearnedInde] Epochs: 200, step: 20\n",
      "[2023-07-14 15:12:07,214][INFO ][__main__.LearnedInde] Epoch 20 | Loss 1.23865\n",
      "[2023-07-14 15:12:28,505][INFO ][__main__.LearnedInde] Epoch 40 | Loss 0.85032\n"
     ]
    }
   ],
   "source": [
    "li = LearnedIndex()\n",
    "pred_categories, build_t = li.build(\n",
    "    data,\n",
    "    n_categories=100,\n",
    "    epochs=200,\n",
    "    lr=0.1\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8136412-82b3-493c-90e9-51f663a8de29",
   "metadata": {},
   "outputs": [],
   "source": [
    "k=10\n",
    "bucket=8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f87d9f08-3832-4bc8-800b-3d2834c996ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "dists, nns, search_t, inference_t, search_single_t, seq_search_t, pure_seq_search_t, sort_t = li.search(\n",
    "    data_navigation=data,\n",
    "    queries_navigation=queries,\n",
    "    data_search=data_search,\n",
    "    queries_search=queries_search,\n",
    "    pred_categories=pred_categories,\n",
    "    n_buckets=bucket,\n",
    "    k=k,\n",
    "    use_threshold=True\n",
    ")\n",
    "LOG.info('Inference time: %s', inference_t)\n",
    "LOG.info('Search time: %s', search_t)\n",
    "LOG.info('Search single time: %s', search_single_t)\n",
    "LOG.info('Sequential search time: %s', seq_search_t)\n",
    "LOG.info('Pure sequential search time: %s', pure_seq_search_t)\n",
    "LOG.info('Sort time: %s', sort_t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81efdfc0-1119-4326-8ea0-612dc459cfcf",
   "metadata": {},
   "outputs": [],
   "source": [
    "recall = get_recall(nns, loaded_gt, 10)\n",
    "recall"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a19e12b-480b-455d-b636-a1368a8e4d89",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
