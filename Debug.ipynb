{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "446e9f82-522b-4eb7-afe2-1c5a3decf63f",
   "metadata": {},
   "outputs": [],
   "source": [
    "%reload_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "65c32811-5d80-4cf4-87c9-b3f57a0f0a6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import h5py\n",
    "import pandas as pd\n",
    "import pickle\n",
    "from tqdm import tqdm\n",
    "from li.utils import pairwise_cosine\n",
    "import time\n",
    "import logging\n",
    "import numpy as np\n",
    "import os\n",
    "from scipy import sparse\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ee6582d4-9825-4ca7-aa66-9e40ffc59f6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "logging.basicConfig(\n",
    "    level=logging.INFO,\n",
    "    format='[%(asctime)s][%(levelname)-5.5s][%(name)-.20s] %(message)s'\n",
    ")\n",
    "LOG = logging.getLogger(__name__)\n",
    "\n",
    "def increase_max_recursion_limit():\n",
    "    \"\"\" Increases the maximum recursion limit.\n",
    "    Source: https://stackoverflow.com/a/16248113\n",
    "    \"\"\"\n",
    "    import sys\n",
    "    import resource\n",
    "    resource.setrlimit(resource.RLIMIT_STACK, (2**29, -1))\n",
    "    sys.setrecursionlimit(10**6)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "92f0025a-bc1a-4455-bd0b-e66299147ae8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from li.Logger import Logger\n",
    "from li.utils import pairwise_cosine\n",
    "import time\n",
    "from sklearn.metrics import accuracy_score\n",
    "import torch\n",
    "import torch.utils.data\n",
    "#from li.model import NeuralNetwork, data_X_to_torch, LIDataset\n",
    "import faiss\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "import torch.nn.functional as nnf\n",
    "from dlmi.utils import get_device, reverse_dict\n",
    "import numpy as np\n",
    "from dlmi.Logger import Logger\n",
    "from typing import List, Tuple\n",
    "import torch\n",
    "import torch.utils.data\n",
    "\n",
    "\n",
    "class Model(nn.Module):\n",
    "    def __init__(self, input_dim=768, output_dim=1000, model_type=None):\n",
    "        super().__init__()\n",
    "        if model_type == 'MLP':\n",
    "            self.layers = torch.nn.Sequential(\n",
    "            torch.nn.Linear(input_dim, 128),\n",
    "            torch.nn.ReLU(),\n",
    "            torch.nn.Linear(128, output_dim)\n",
    "            )\n",
    "        if model_type == 'Bigger':\n",
    "            self.layers = torch.nn.Sequential(\n",
    "            torch.nn.Linear(input_dim, 256),\n",
    "            torch.nn.ReLU(),\n",
    "            torch.nn.Linear(256, 128),\n",
    "            torch.nn.ReLU(),\n",
    "            torch.nn.Linear(128, output_dim)\n",
    "            )\n",
    "        self.n_output_neurons = output_dim\n",
    "\n",
    "    def forward(self, x: torch.FloatTensor) -> torch.FloatTensor:\n",
    "        outputs = self.layers(x)\n",
    "        return outputs\n",
    "\n",
    "\n",
    "def get_device() -> torch.device:\n",
    "    \"\"\" Gets the `device` to be used by torch.\n",
    "    This arugment is needed to operate with the PyTorch model instance.\n",
    "\n",
    "    Returns\n",
    "    ------\n",
    "    torch.device\n",
    "        Device\n",
    "    \"\"\"\n",
    "    use_cuda = torch.cuda.is_available()\n",
    "    device = torch.device('cuda:0' if use_cuda else 'cpu')\n",
    "    torch.backends.cudnn.benchmark = True\n",
    "    return device\n",
    "\n",
    "\n",
    "def data_X_to_torch(data) -> torch.FloatTensor:\n",
    "    \"\"\" Creates torch training data.\"\"\"\n",
    "    data_X = torch.from_numpy(np.array(data).astype(np.float32))\n",
    "    return data_X\n",
    "\n",
    "\n",
    "def data_to_torch(data, labels) -> Tuple[torch.FloatTensor, torch.LongTensor]:\n",
    "    \"\"\" Creates torch training data and labels.\"\"\"\n",
    "    data_X = data_X_to_torch(data)\n",
    "    data_y = torch.as_tensor(torch.from_numpy(labels), dtype=torch.long)\n",
    "    return data_X, data_y\n",
    "\n",
    "\n",
    "class NeuralNetwork(Logger):\n",
    "    \"\"\" The neural network class corresponding to every inner node.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    input_dim : int\n",
    "        The input dimension.\n",
    "    output_dim : int\n",
    "        The output dimension.\n",
    "    loss : torch.nn, optional\n",
    "        The loss function, the default is torch.nn.CrossEntropyLoss.\n",
    "    lr : float, optional\n",
    "        The learning rate, the default is 0.001.\n",
    "    model_type : str, optional\n",
    "        The model type, the default is 'MLP'.\n",
    "    class_weight : torch.FloatTensor, optional\n",
    "        The class weights, the default is None.\n",
    "    \"\"\"\n",
    "    def __init__(\n",
    "        self,\n",
    "        input_dim,\n",
    "        output_dim,\n",
    "        loss=torch.nn.CrossEntropyLoss,\n",
    "        lr=0.1,\n",
    "        model_type='MLP',\n",
    "        class_weight=None\n",
    "    ):\n",
    "        self.device = get_device()\n",
    "        self.model = Model(input_dim, output_dim, model_type=model_type).to(self.device)\n",
    "        if not isinstance(class_weight, type(None)):\n",
    "            self.loss = loss(weight=class_weight.to(self.device))\n",
    "        else:\n",
    "            self.loss = loss()\n",
    "        self.optimizer = torch.optim.Adam(self.model.parameters(), lr=lr)\n",
    "\n",
    "    def train(\n",
    "        self,\n",
    "        data_X: torch.FloatTensor,\n",
    "        data_y: torch.LongTensor,\n",
    "        epochs=500,\n",
    "        logger=None\n",
    "    ):\n",
    "        #logger.debug(f'Epochs: {epochs}')\n",
    "        step = epochs // 10\n",
    "        losses = []\n",
    "        if logger:\n",
    "            logger.info(f'Epochs: {epochs}, step: {step}')\n",
    "        for ep in range(epochs):\n",
    "            pred_y = self.model(data_X.to(self.device))\n",
    "            curr_loss = self.loss(pred_y, data_y.to(self.device))\n",
    "            if ep % step == 0 and ep != 0:\n",
    "                if logger:\n",
    "                    logger.info(f'Epoch {ep} | Loss {curr_loss.item()}')\n",
    "            losses.append(curr_loss.item())\n",
    "\n",
    "            self.model.zero_grad()\n",
    "            curr_loss.backward()\n",
    "\n",
    "            self.optimizer.step()\n",
    "        return losses\n",
    "\n",
    "    def train_batch(\n",
    "        self,\n",
    "        dataset,\n",
    "        epochs=5,\n",
    "        logger=None\n",
    "    ):\n",
    "        #logger.debug(f'Epochs: {epochs}')\n",
    "        step = epochs // 10\n",
    "        step = step if step > 0 else 1\n",
    "        losses = []\n",
    "        if logger:\n",
    "            logger.info(f'Epochs: {epochs}, step: {step}')\n",
    "        for ep in range(epochs):\n",
    "            for data_X, data_y in iter(dataset):\n",
    "                pred_y = self.model(data_X.to(self.device))\n",
    "                curr_loss = self.loss(pred_y, data_y.to(self.device))\n",
    "\n",
    "            if ep % step == 0 and ep != 0:\n",
    "                if logger:\n",
    "                    logger.info(f'Epoch {ep} | Loss {curr_loss.item():.5f}')\n",
    "            losses.append(curr_loss.item())\n",
    "\n",
    "            self.model.zero_grad()\n",
    "            curr_loss.backward()\n",
    "\n",
    "            self.optimizer.step()\n",
    "        return losses\n",
    "\n",
    "    def predict(self, data_X: torch.FloatTensor):\n",
    "        \"\"\" Collects predictions for multiple data points (used in structure building).\"\"\"\n",
    "        self.model = self.model.to(self.device)\n",
    "        self.model.eval()\n",
    "\n",
    "        all_outputs = torch.tensor([], device=self.device)\n",
    "        with torch.no_grad():\n",
    "            outputs = self.model(data_X.to(self.device))\n",
    "            all_outputs = torch.cat((all_outputs, outputs), 0)\n",
    "\n",
    "        _, y_pred = torch.max(all_outputs, 1)\n",
    "        return y_pred.cpu().numpy()\n",
    "\n",
    "    def predict_proba(self, data_X: torch.FloatTensor):\n",
    "        \"\"\" Collects predictions for a single data point (used in query predictions).\"\"\"\n",
    "        self.model = self.model.to(self.device)\n",
    "        self.model.eval()\n",
    "\n",
    "        with torch.no_grad():\n",
    "            outputs = self.model(data_X.to(self.device))\n",
    "\n",
    "        if outputs.dim() == 1:\n",
    "            dim=0\n",
    "        else:\n",
    "            dim=1\n",
    "        prob = nnf.softmax(outputs, dim=dim)\n",
    "\n",
    "        #if prob.dim() == 1:\n",
    "        #    probs, classes = prob.topk(prob.shape[0])\n",
    "        #else:\n",
    "        probs, classes = prob.topk(prob.shape[1])\n",
    "\n",
    "        return probs.cpu().numpy(), classes.cpu().numpy()\n",
    "\n",
    "\n",
    "class LIDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, dataset_x, dataset_y):\n",
    "        self.dataset_x, self.dataset_y = data_to_torch(dataset_x, dataset_y)\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.dataset_x.shape[0]\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        return self.dataset_x[idx-1], self.dataset_y[idx-1]\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "class LearnedIndex(Logger):\n",
    "\n",
    "    def __init__(self):\n",
    "        self.pq = []\n",
    "        self.model = None\n",
    "\n",
    "    def search(\n",
    "        self,\n",
    "        data_navigation,\n",
    "        queries_navigation,\n",
    "        data_search,\n",
    "        queries_search,\n",
    "        pred_categories,\n",
    "        n_buckets=1,\n",
    "        k=10\n",
    "    ):\n",
    "        \"\"\" Search for k nearest neighbors for each query in queries.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        queries : np.array\n",
    "            Queries to search for.\n",
    "        data : np.array\n",
    "            Data to search in.\n",
    "        n_buckets : int\n",
    "            Number of most similar buckets to search in.\n",
    "        k : int\n",
    "            Number of nearest neighbors to search for.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        dists : np.array\n",
    "            Array of shape (queries.shape[0], k) with distances to nearest neighbors for each query.\n",
    "        anns : np.array\n",
    "            Array of shape (queries.shape[0], k) with nearest neighbors for each query.\n",
    "        time : float\n",
    "            Time it took to search.\n",
    "        \"\"\"\n",
    "        assert self.model is not None, 'Model is not trained, call `build` first.'\n",
    "        s = time.time()\n",
    "        _, pred_proba_categories = self.model.predict_proba(\n",
    "            data_X_to_torch(queries_navigation)\n",
    "        )\n",
    "        anns_final = None\n",
    "        dists_final = None\n",
    "        # sorts the predictions of a bucket for each query, ordered by lowest probability\n",
    "        data_navigation['category'] = pred_categories\n",
    "\n",
    "        # iterates over the predicted buckets starting from the most similar (index -1)\n",
    "        for bucket in range(n_buckets):\n",
    "            dists, anns = self.search_single(\n",
    "                data_navigation,\n",
    "                data_search,\n",
    "                queries_search,\n",
    "                pred_proba_categories[:, bucket]\n",
    "            )\n",
    "            if anns_final is None:\n",
    "                anns_final = anns\n",
    "                dists_final = dists\n",
    "            else:\n",
    "                # stacks the results from the previous sorted anns and dists\n",
    "                # *_final arrays now have shape (queries.shape[0], k*2)\n",
    "                anns_final = np.hstack((anns_final, anns))\n",
    "                dists_final = np.hstack((dists_final, dists))\n",
    "                # gets the sorted indices of the stacked dists\n",
    "                idx_sorted = dists_final.argsort(kind='stable', axis=1)[:, :k]\n",
    "                # indexes the final arrays with the sorted indices\n",
    "                # *_final arrays now have shape (queries.shape[0], k)\n",
    "                idx = np.ogrid[tuple(map(slice, dists_final.shape))]\n",
    "                idx[1] = idx_sorted\n",
    "                dists_final = dists_final[tuple(idx)]\n",
    "                anns_final = anns_final[tuple(idx)]\n",
    "\n",
    "                assert anns_final.shape == dists_final.shape == (queries_search.shape[0], k)\n",
    "\n",
    "        return dists_final, anns_final, time.time() - s\n",
    "\n",
    "    def search_single(\n",
    "        self,\n",
    "        data_navigation,\n",
    "        data_search,\n",
    "        queries_search,\n",
    "        pred_categories,\n",
    "        k=10\n",
    "    ):\n",
    "        \"\"\" Search for k nearest neighbors for each query in queries.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        queries : np.array\n",
    "            Queries to search for.\n",
    "        data : np.array\n",
    "            Data to search in.\n",
    "        k : int\n",
    "            Number of nearest neighbors to search for.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        anns : np.array\n",
    "            Array of shape (queries.shape[0], k) with nearest neighbors for each query.\n",
    "        final_dists_k : np.array\n",
    "            Array of shape (queries.shape[0], k) with distances to nearest neighbors for each query.\n",
    "        time : float\n",
    "            Time it took to search.\n",
    "        \"\"\"\n",
    "        nns = np.zeros((queries_search.shape[0], k), dtype=np.uint32)\n",
    "        dists = np.zeros((queries_search.shape[0], k), dtype=np.float32)\n",
    "\n",
    "        for cat in np.unique(pred_categories):\n",
    "            cat_idxs = np.where(pred_categories == cat)[0]\n",
    "            bucket_obj_indexes = data_navigation.query('category == @cat').index\n",
    "            #if bucket_obj_indexes.shape[0] != 0:\n",
    "            seq_search_dists = pairwise_cosine(\n",
    "                queries_search[cat_idxs], data_search.loc[bucket_obj_indexes]\n",
    "            )\n",
    "            ann_relative = seq_search_dists.argsort()[:, :k]\n",
    "            nns[cat_idxs] = np.array(bucket_obj_indexes)[ann_relative]\n",
    "            dists[cat_idxs] = np.take_along_axis(seq_search_dists, ann_relative, axis=1)\n",
    "\n",
    "        return dists, nns\n",
    "\n",
    "    def build(self, data, n_categories=100, epochs=100, lr=0.1):\n",
    "        \"\"\" Build the index.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        data : np.array\n",
    "            Data to build the index on.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        time : float\n",
    "            Time it took to build the index.\n",
    "        \"\"\"\n",
    "        s = time.time()\n",
    "        # ---- cluster the data into categories ---- #\n",
    "        _, labels = self.cluster(data, n_categories)\n",
    "\n",
    "        # ---- train a neural network ---- #\n",
    "        dataset = LIDataset(data, labels)\n",
    "        train_loader = torch.utils.data.DataLoader(\n",
    "            dataset,\n",
    "            batch_size=256,\n",
    "            sampler=torch.utils.data.SubsetRandomSampler(\n",
    "                data.index.values.tolist()\n",
    "            )\n",
    "        )\n",
    "        nn = NeuralNetwork(\n",
    "            input_dim=data.shape[1],\n",
    "            output_dim=n_categories,\n",
    "            lr=lr,\n",
    "            model_type='MLP'\n",
    "        )\n",
    "        nn.train_batch(train_loader, epochs=epochs, logger=self.logger)\n",
    "        # ---- collect predictions ---- #\n",
    "        self.model = nn\n",
    "        return nn.predict(data_X_to_torch(data)), time.time() - s\n",
    "\n",
    "    def cluster(self, data, n_clusters):\n",
    "        if data.shape[0] < 2:\n",
    "            return None, np.zeros_like(data.shape[0])\n",
    "\n",
    "        if data.shape[0] < n_clusters:\n",
    "            n_clusters = data.shape[0] // 5\n",
    "            if n_clusters < 2:\n",
    "                n_clusters = 2\n",
    "\n",
    "        kmeans = faiss.Kmeans(d=np.array(data).shape[1], k=n_clusters)\n",
    "        X = np.array(data).astype(np.float32)\n",
    "        kmeans.train(X)\n",
    "\n",
    "        return kmeans, kmeans.index.search(X, 1)[1].T[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7519d7bc-60c8-4946-923a-9730a886d5b9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2023-07-06 17:01:43,945][INFO ][__main__] Loading pca32 data\n",
      "[2023-07-06 17:01:44,164][INFO ][__main__] Loading queries\n",
      "[2023-07-06 17:01:44,626][INFO ][__main__] Loading clip data\n",
      "[2023-07-06 17:01:46,290][INFO ][__main__] Loading GT\n"
     ]
    }
   ],
   "source": [
    "size = '100K'\n",
    "\n",
    "LOG.info(f'Loading pca32 data')\n",
    "data_path = f'data/pca32v2/{size}/dataset.h5'\n",
    "f = h5py.File(data_path, 'r')\n",
    "loaded_data = f['pca32'][:, :]\n",
    "data = pd.DataFrame(loaded_data)\n",
    "data.index += 1\n",
    "\n",
    "LOG.info(f'Loading queries')\n",
    "base_path = f'data/pca32v2/{size}/'\n",
    "queries_path = f'{base_path}/query.h5'\n",
    "f2 = h5py.File(queries_path, 'r')\n",
    "#loaded_queries = f2['emb'][:, :]\n",
    "loaded_queries = f2['pca32'][:, :]\n",
    "\n",
    "base_path = f'data/clip768v2/{size}/'\n",
    "queries_path = f'{base_path}/query.h5'\n",
    "f2 = h5py.File(queries_path, 'r')\n",
    "#loaded_queries = f2['emb'][:, :]\n",
    "loaded_queries_seq = f2['emb'][:, :]\n",
    "\n",
    "LOG.info(f'Loading clip data')\n",
    "data_path = f'data/clip768v2/{size}/dataset.h5'\n",
    "f = h5py.File(data_path, 'r')\n",
    "loaded_clip_data = f['emb'][:, :]\n",
    "loaded_clip_data = pd.DataFrame(loaded_clip_data)\n",
    "loaded_clip_data.index += 1\n",
    "\n",
    "LOG.info(f'Loading GT')\n",
    "gt_path = f'data/groundtruth-{size}.h5'\n",
    "f3 = h5py.File(gt_path, 'r')\n",
    "loaded_gt = f3['knns'][:, :]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "7c75473b-178b-4528-9e4b-3a03db32576b",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_categories=100\n",
    "epochs=100\n",
    "lr=0.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a45d41f7-b0c6-4d02-aeb6-2985e8a81dcc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/auto/brno12-cerit/nfs4/home/tslaninakova/sisap-challenge-clean'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "os.path.abspath(\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "57ae99e9-06ae-4e45-a82c-cea1384498e6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2023-07-06 17:06:45,386][INFO ][__main__.LearnedInde] Epochs: 100, step: 10\n",
      "[2023-07-06 17:06:58,380][INFO ][__main__.LearnedInde] Epoch 10 | Loss 2.12920\n",
      "[2023-07-06 17:07:10,147][INFO ][__main__.LearnedInde] Epoch 20 | Loss 1.11942\n",
      "[2023-07-06 17:07:21,924][INFO ][__main__.LearnedInde] Epoch 30 | Loss 1.08467\n",
      "[2023-07-06 17:07:33,673][INFO ][__main__.LearnedInde] Epoch 40 | Loss 0.79363\n",
      "[2023-07-06 17:07:45,422][INFO ][__main__.LearnedInde] Epoch 50 | Loss 0.84400\n",
      "[2023-07-06 17:07:57,226][INFO ][__main__.LearnedInde] Epoch 60 | Loss 0.62600\n",
      "[2023-07-06 17:08:08,967][INFO ][__main__.LearnedInde] Epoch 70 | Loss 0.84457\n",
      "[2023-07-06 17:08:20,714][INFO ][__main__.LearnedInde] Epoch 80 | Loss 0.74277\n",
      "[2023-07-06 17:08:32,440][INFO ][__main__.LearnedInde] Epoch 90 | Loss 0.72751\n",
      "[2023-07-06 17:08:43,202][INFO ][__main__] Pure build time: 118.17009544372559\n"
     ]
    }
   ],
   "source": [
    "li = LearnedIndex()\n",
    "# ---- build the index ---- #\n",
    "pred_categories, build_t = li.build(\n",
    "    data,\n",
    "    n_categories=n_categories,\n",
    "    epochs=epochs,\n",
    "    lr=lr\n",
    ")\n",
    "LOG.info(f'Pure build time: {build_t}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "7609b5c4-b914-404a-8b33-75ea32f29fe1",
   "metadata": {},
   "outputs": [],
   "source": [
    "probs, pred_proba_categories = li.model.predict_proba(\n",
    "    data_X_to_torch(loaded_queries)\n",
    ")\n",
    "anns_final = None\n",
    "dists_final = None\n",
    "# sorts the predictions of a bucket for each query, ordered by lowest probability\n",
    "data['category'] = pred_categories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "5f2416c4-4267-4d25-bbb0-6dcf2224dca1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10000, 100)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred_proba_categories.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "195a6cb5-809a-476a-9bd5-bc4dec3e8fb3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2023-07-06 17:09:51,809][INFO ][numexpr.utils] Note: detected 128 virtual cores but NumExpr set to maximum of 64, check \"NUMEXPR_MAX_THREADS\" environment variable.\n",
      "[2023-07-06 17:09:51,810][INFO ][numexpr.utils] Note: NumExpr detected 128 cores but \"NUMEXPR_MAX_THREADS\" not set, so enforcing safe limit of 8.\n"
     ]
    }
   ],
   "source": [
    "dists, anns = li.search_single(\n",
    "    data,\n",
    "    loaded_clip_data,\n",
    "    loaded_queries_seq,\n",
    "    pred_proba_categories[:, 0]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "50ff49f2-b457-4133-a843-aabd2dcd5c9a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10000, 10)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "anns.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "efe7d5ed-08ab-44c7-8d9b-b68f43a3db13",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([47, 56, 51, 60, 53, 35, 55, 41, 32, 89, 71, 80, 70, 85, 68, 49, 84,\n",
       "       86, 30, 23, 45, 83, 82, 92, 87,  5, 69, 43, 38, 36, 99, 29, 27, 75,\n",
       "       26, 34, 94,  2, 95, 25, 48, 97, 21, 46, 14, 31,  8, 20, 73, 24, 78,\n",
       "       57,  0, 44, 59,  6, 88, 62, 19, 81, 98, 22, 93, 58, 77, 96, 40, 33,\n",
       "        3, 39,  9, 67, 74,  1, 11, 15, 18, 52, 42, 28, 37, 16, 50, 91,  4,\n",
       "       72, 63, 66, 76, 61, 54, 90, 12, 64, 79, 13, 65, 10,  7, 17])"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred_proba_categories[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "0d95e91c-b231-4e05-8013-ea1d1d5c19ae",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "47.0"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.loc[92811].category"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "b12e486a-2a1d-403a-bc7c-0dfe850012d1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([47, 54, 59, ..., 96, 32, 75])"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred_proba_categories[:, 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "46063432-2ab5-4caa-91dd-23a9d9591b4d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([79172, 15735, 22337,   231, 74173, 41079, 38159, 71849, 69015,\n",
       "       92811], dtype=int32)"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loaded_gt[0][:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "c05d03d4-cbe1-4220-a3eb-e67e4caf1e16",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([15735, 92811, 87457, 42586, 70297,  5451, 21472, 87037,  6911,\n",
       "       45784], dtype=uint32)"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "anns[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "7e3f2999-07c0-4b79-afcc-e304adde6060",
   "metadata": {},
   "outputs": [],
   "source": [
    "dists2, anns2 = li.search_single(\n",
    "    data,\n",
    "    loaded_clip_data,\n",
    "    loaded_queries_seq,\n",
    "    pred_proba_categories[:, 1]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "43248497-b277-417c-a8ad-dfd3127d968b",
   "metadata": {},
   "outputs": [],
   "source": [
    "anns_final = anns\n",
    "dists_final = dists"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "8d693646-30d0-40cc-8cd4-769c3b115cf0",
   "metadata": {},
   "outputs": [],
   "source": [
    "anns_final = np.hstack((anns_final, anns2))\n",
    "dists_final = np.hstack((dists_final, dists2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "7cc525f3-b051-49b8-964e-de2bb77b8f47",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([10,  0, 11, 12, 13, 14, 15,  1, 16, 17])"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "k=10\n",
    "idx_sorted = dists_final.argsort(kind='stable', axis=1)[:, :k]\n",
    "idx_sorted[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "45108627-40b7-475e-8481-f2bf17b47cfd",
   "metadata": {},
   "outputs": [],
   "source": [
    "idx = np.ogrid[tuple(map(slice, dists_final.shape))]\n",
    "idx[1] = idx_sorted\n",
    "dists_final = dists_final[tuple(idx)]\n",
    "anns_final = anns_final[tuple(idx)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "9e451d24-1503-4deb-88ea-03aa6c9a900f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([79172, 15735, 22337, 74173, 41079, 38159, 69015, 92811, 99973,\n",
       "       79896], dtype=uint32)"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "anns_final[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "3015877a-0b64-4b2f-bab8-d97229d1f5ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_buckets=99"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "e8558dba-3450-4c2d-ba44-60dc009135c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_navigation = data\n",
    "data_search = loaded_clip_data\n",
    "queries_search = loaded_queries_seq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "d7fec22d-3e38-4bf9-995f-9298381ce81f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 2min 53s, sys: 366 ms, total: 2min 53s\n",
      "Wall time: 2min 57s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "anns_final = None\n",
    "dists_final = None\n",
    "for bucket in range(n_buckets):\n",
    "    dists, anns = li.search_single(\n",
    "        data_navigation,\n",
    "        data_search,\n",
    "        queries_search,\n",
    "        pred_proba_categories[:, bucket]\n",
    "    )\n",
    "    if anns_final is None:\n",
    "        anns_final = anns\n",
    "        dists_final = dists\n",
    "    else:\n",
    "        # stacks the results from the previous sorted anns and dists\n",
    "        # *_final arrays now have shape (queries.shape[0], k*2)\n",
    "        anns_final = np.hstack((anns_final, anns))\n",
    "        dists_final = np.hstack((dists_final, dists))\n",
    "        # gets the sorted indices of the stacked dists\n",
    "        idx_sorted = dists_final.argsort(kind='stable', axis=1)[:, :k]\n",
    "        # indexes the final arrays with the sorted indices\n",
    "        # *_final arrays now have shape (queries.shape[0], k)\n",
    "        idx = np.ogrid[tuple(map(slice, dists_final.shape))]\n",
    "        idx[1] = idx_sorted\n",
    "        dists_final = dists_final[tuple(idx)]\n",
    "        anns_final = anns_final[tuple(idx)]\n",
    "\n",
    "        assert anns_final.shape == dists_final.shape == (queries_search.shape[0], k)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "4331af68-3d8e-4701-afc2-33999ae4783e",
   "metadata": {},
   "outputs": [],
   "source": [
    "overlaps = []\n",
    "for i in range(10_000):\n",
    "    overlaps.append(np.intersect1d(anns_final[i], loaded_gt[i]).shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "8b43967c-b29d-48b6-9eb4-43e74d80a768",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "9.9936"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.mean(overlaps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a3d2284-82b8-4f9a-bcd8-2e73d8dc0784",
   "metadata": {},
   "outputs": [],
   "source": [
    "anns_final[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "90d2caaf-2738-4311-8dea-35eec2bf319b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "        True])"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "anns_final[0] == loaded_gt[0][:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "298dd6ab-611d-48bb-8d63-5721ac6397b6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c3e996d-ab25-4f1d-b212-6f55e0e4d6c2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
